{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567ceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "#         metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        temperature_h,\n",
    "        temperature_e,\n",
    "        alpha_h,\n",
    "        alpha_e\n",
    "\n",
    "    ):\n",
    "\n",
    "        super().compile(optimizer=optimizer,metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha_h = alpha_h\n",
    "        self.alpha_e = alpha_e\n",
    "        self.temperature_h = temperature_h\n",
    "        self.temperature_e = temperature_e\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, [y_h,y_e] = data\n",
    "        print('x.shape',x.shape.as_list())\n",
    "        print('y_h.shape',y_h.shape.as_list())\n",
    "        teacher_prediction_h, teacher_prediction_e= self.teacher(x, training=False)\n",
    "        print('teacher_prediction_h.shape',teacher_prediction_h.shape.as_list())\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_prediction_h,student_prediction_e = self.student(x, training=True)\n",
    "            print('student_prediction_h.shape',student_prediction_h.shape.as_list())\n",
    "            # Compute losses\n",
    "            student_loss_h = self.student_loss_fn(y_h, student_prediction_h)\n",
    "            student_loss_e = self.student_loss_fn(y_e, student_prediction_e)\n",
    "            # The magnitudes of the gradients produced by the soft targets scale\n",
    "            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n",
    "            distillation_loss_h = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_prediction_h / self.temperature_h, axis=-1),\n",
    "                    tf.nn.softmax(student_prediction_h / self.temperature_h, axis=-1),\n",
    "                )\n",
    "                * self.temperature_h**2\n",
    "            )\n",
    "            distillation_loss_e = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_prediction_e / self.temperature_e, axis=-1),\n",
    "                    tf.nn.softmax(student_prediction_e / self.temperature_e, axis=-1),\n",
    "                )\n",
    "                * self.temperature_e**2\n",
    "            )\n",
    "\n",
    "            loss_h = self.alpha_h * student_loss_h + (1 - self.alpha_h) * distillation_loss_h\n",
    "            loss_e = self.alpha_e * student_loss_e + (1 - self.alpha_e) * distillation_loss_e\n",
    "            loss=0.5*loss_h+0.5*loss_e\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "\n",
    "        self.compiled_metrics.update_state([y_h,y_e], [student_prediction_h,student_prediction_e])\n",
    "      \n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"loss\": loss})\n",
    "        return results      \n",
    "    \n",
    "   \n",
    "    def predict(self,db_test_x,batchz=None,verbose=True):\n",
    "\n",
    "        y_pre_h = np.array([])\n",
    "        y_pre_e = np.array([])\n",
    "\n",
    "        db_test_x=tf.data.Dataset.from_tensor_slices(db_test_x)\n",
    "    \n",
    "        for elem in db_test_x.as_numpy_iterator():\n",
    "            elem=tf.reshape(  elem, [1,500,4])\n",
    "            batch_y_pre_h,batch_y_pre_e=self.student(elem,training=False)\n",
    "            batch_y_pre_h=tf.nn.softmax(batch_y_pre_h,axis=-1)\n",
    "            batch_y_pre_e=tf.nn.softmax(batch_y_pre_e,axis=-1)\n",
    "            batch_y_pre_h=np.argmax(batch_y_pre_h,axis=-1)\n",
    "            batch_y_pre_e=np.argmax(batch_y_pre_e,axis=-1)      \n",
    "            y_pre_h = np.insert(y_pre_h, len(y_pre_h), batch_y_pre_h)\n",
    "            y_pre_e = np.insert(y_pre_e, len(y_pre_e), batch_y_pre_e)\n",
    "\n",
    "#             y_tru_h = np.insert(y_tru_h, len(y_tru_h), batch_y_tru_h)\n",
    "#             y_tru_e = np.insert(y_tru_e, len(y_tru_e), batch_y_tru_e)\n",
    "#         if verbose:\n",
    "#             for pre_h,pre_e in zip(y_pre_h,y_pre_e):\n",
    "#                 print(f\" hand预测值{pre_h}---->elbow预测值{pre_e}\")\n",
    "        return y_pre_h,y_pre_e\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "# Create the teacher\n",
    "batch_size = 128       # Batch size\n",
    "seq_len = 500     # Number of steps 128\n",
    "#learning_rate = 0.0001\n",
    "epochs = 200\n",
    "n_channels = 4\n",
    "n_class = 2\n",
    "\n",
    "\n",
    "inputs_1 = keras.layers.Input((seq_len, n_channels))\n",
    "conv_1=keras.layers.Conv1D(filters= 18, kernel_size=2, padding='Same', activation='relu', kernel_regularizer=l2(1e-3))(inputs_1)\n",
    "norm_1=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_1)\n",
    "max_1=keras.layers.MaxPool1D(pool_size=2)(norm_1)\n",
    "\n",
    "conv_2=keras.layers.Conv1D(filters= 36, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_1)\n",
    "norm_2=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_2)\n",
    "max_2=keras.layers.MaxPool1D(pool_size=2)(norm_2)\n",
    "\n",
    "conv_3=keras.layers.Conv1D(filters= 72, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_2)\n",
    "conv_3_1=keras.layers.Conv1D(filters= 72, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(conv_3)\n",
    "norm_3=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_3_1)\n",
    "max_3=keras.layers.MaxPool1D(pool_size=2)(norm_3)\n",
    "\n",
    "conv_4=keras.layers.Conv1D(filters= 144, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_3)\n",
    "conv_4_1=keras.layers.Conv1D(filters= 144, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(conv_4)\n",
    "norm_4=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_4_1)\n",
    "max_4=keras.layers.MaxPool1D(pool_size=2)(norm_4)\n",
    "flatten_1=keras.layers.Flatten()(max_4)\n",
    "\n",
    "fc_1=keras.layers.Dense(144,kernel_regularizer=l2(1e-3))(flatten_1)\n",
    "\n",
    "output_1=keras.layers.Dense(3,activation=None)(fc_1)\n",
    "output_2=keras.layers.Dense(3,activation=None)(fc_1)\n",
    "\n",
    "\n",
    "\n",
    "teacher= keras.Model(inputs=inputs_1, outputs=[output_1,output_2])\n",
    "\n",
    "\n",
    "# # Create the student\n",
    "inputs_1_s = keras.layers.Input((seq_len, n_channels))\n",
    "conv_1_s=keras.layers.Conv1D(filters= 18, kernel_size=2, padding='Same', activation='relu', kernel_regularizer=l2(1e-3))(inputs_1_s)\n",
    "norm_1_s=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_1_s)\n",
    "max_1_s=keras.layers.MaxPool1D(pool_size=2)(norm_1_s)\n",
    "\n",
    "conv_2_s=keras.layers.Conv1D(filters= 36, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_1_s)\n",
    "norm_2_s=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_2_s)\n",
    "max_2_s=keras.layers.MaxPool1D(pool_size=2)(norm_2_s)\n",
    "\n",
    "conv_3_s=keras.layers.Conv1D(filters= 72, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_2_s)\n",
    "conv_3_1_s=keras.layers.Conv1D(filters= 72, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(conv_3_s)\n",
    "norm_3_s=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_3_1_s)\n",
    "max_3_s=keras.layers.MaxPool1D(pool_size=2)(norm_3_s)\n",
    "\n",
    "conv_4_s=keras.layers.Conv1D(filters= 144, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(max_3_s)\n",
    "conv_4_1_s=keras.layers.Conv1D(filters= 144, kernel_size=2, padding='Same', activation='relu',kernel_regularizer=l2(1e-3))(conv_4_s)\n",
    "norm_4_s=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv_4_1_s)\n",
    "max_4_s=keras.layers.MaxPool1D(pool_size=2)(norm_4_s)\n",
    "flatten_1_s=keras.layers.Flatten()(max_4_s)\n",
    "\n",
    "fc_1_s=keras.layers.Dense(144,kernel_regularizer=l2(1e-3))(flatten_1_s)\n",
    "\n",
    "output_1_s=keras.layers.Dense(3,activation=None)(fc_1_s)\n",
    "output_2_s=keras.layers.Dense(3,activation=None)(fc_1_s)\n",
    "\n",
    "\n",
    "student = keras.Model(inputs=inputs_1_s, outputs=[output_1_s,output_2_s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e230ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array.shape (52668, 6)\n",
      "array10000] [0. 0. 0. 0. 0. 0.]\n",
      "array.shape (52668, 6)\n",
      "10434\n",
      "(labels_h[0]) [1.]\n",
      "array.shape (58461, 6)\n",
      "array10000] [0. 0. 0. 0. 0. 0.]\n",
      "array.shape (58461, 6)\n",
      "11593\n",
      "(labels_h[0]) [0.]\n",
      "array.shape (23907, 6)\n",
      "array10000] [0.     0.     0.0201 0.     0.     0.    ]\n",
      "array.shape (23907, 6)\n",
      "4682\n",
      "(labels_h[0]) [0.]\n",
      "array.shape (27774, 6)\n",
      "array10000] [0.     0.     0.0563 0.1648 1.     0.    ]\n",
      "array.shape (27774, 6)\n",
      "5455\n",
      "(labels_h[0]) [0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utilities import *\n",
    "\n",
    "\n",
    "X_test1, labels_test_h1,labels_test_e1 = read_data(data_path='./data_hand_and_elbow/elbow_hand_close_test.txt', split=\"train\") # train\n",
    "X_test2, labels_test_h2,labels_test_e2 = read_data(data_path='./data_hand_and_elbow/elbow_hand_open_test.txt', split=\"train\") # train\n",
    "\n",
    "\n",
    "X_test=np.concatenate((X_test1,X_test2))\n",
    "\n",
    "\n",
    "labels_test_h=np.concatenate((labels_test_h1,labels_test_h2))\n",
    "\n",
    "\n",
    "labels_test_e=np.concatenate((labels_test_e1,labels_test_e2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_target1, labels_test_h_target1,labels_test_e_target1 = read_data(data_path='./data_wok/data_wok_test1.txt', split=\"train\")# train\n",
    "X_test_target2, labels_test_h_target2,labels_test_e_target2 = read_data(data_path='./data_wok/data_wok_test2.txt', split=\"train\") # train\n",
    "\n",
    "    \n",
    "X_test_target=np.concatenate((X_test_target1,X_test_target2))\n",
    "\n",
    "\n",
    "\n",
    "labels_test_h_target=np.concatenate((labels_test_h_target1,labels_test_h_target2))\n",
    "\n",
    "\n",
    "\n",
    "labels_test_e_target=np.concatenate((labels_test_e_target1,labels_test_e_target2))\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d89ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "acc_h_target 0.9060866133964683\n",
      "acc_e_target 0.8406826477261518\n",
      "acc_h_source 0.9706723566531984\n",
      "acc_e_source 0.9489263177010033\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# #import matplotlib.pyplot as plt\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# import matplotlib.pyplot as    plt  \n",
    "from sklearn.metrics import confusion_matrix     \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "#  keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "#from tensorflow import keras\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical         #to one-hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Conv1D,MaxPool1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, Concatenate\n",
    "from keras.regularizers import l2\n",
    "#keras.backend.set_epsilon(1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"  \n",
    "\n",
    "config = ConfigProto()\n",
    "config.allow_soft_placement=True \n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.7 \n",
    "config.gpu_options.allow_growth = True   \n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "MODEL_PATH_LWF2= './checkpoint/model_010' \n",
    "distiller.load_weights(MODEL_PATH_LWF2)\n",
    "distiller.compile(\n",
    "    optimizer='adam',\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(),],\n",
    "    student_loss_fn= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    temperature_h=2,\n",
    "    temperature_e=0.3,\n",
    "    alpha_h=0.4,\n",
    "    alpha_e=0.4,\n",
    "\n",
    ")\n",
    "#save as .h5\n",
    "distiller.student.save(\"./student.h5\")\n",
    "student_=load_model(\"./student.h5\")\n",
    "student_.compile(optimizer='adam', loss = {'output_1':'sparse_categorical_crossentropy','output_2':'sparse_categorical_crossentropy'},)\n",
    "\n",
    "\n",
    "\n",
    "pre_h,pre_e=student_(X_test_target,training=False)\n",
    "pre_h=tf.nn.softmax(pre_h,axis=-1)\n",
    "pre_h=np.argmax(pre_h,axis=-1)\n",
    "pre_h=pre_h.reshape((len(pre_h),1))\n",
    "acc_h=accuracy_score(labels_test_h_target,pre_h)\n",
    "print(\"acc_h_target\",acc_h)\n",
    "\n",
    "pre_e=tf.nn.softmax(pre_e,axis=-1)\n",
    "pre_e=np.argmax(pre_e,axis=-1)\n",
    "pre_e=pre_e.reshape((len(pre_e),1))\n",
    "acc_e=accuracy_score(labels_test_e_target,pre_e)\n",
    "print(\"acc_e_target\",acc_e)\n",
    "\n",
    "\n",
    "\n",
    "pre_h,pre_e=student_.predict(X_test)\n",
    "pre_h=tf.nn.softmax(pre_h,axis=-1)\n",
    "# pre_h=np.array(pre_h)\n",
    "pre_h=np.argmax(pre_h,axis=1)\n",
    "pre_h=pre_h.reshape((len(pre_h),1))\n",
    "acc_h=accuracy_score(labels_test_h,pre_h)\n",
    "print(\"acc_h_source\",acc_h)\n",
    "\n",
    "pre_e=tf.nn.softmax(pre_e,axis=-1)\n",
    "# pre_e=np.array(pre_e)\n",
    "pre_e=np.argmax(pre_e,axis=1)\n",
    "pre_e=pre_e.reshape((len(pre_e),1))\n",
    "acc_e=accuracy_score(labels_test_e,pre_e)\n",
    "print(\"acc_e_source\",acc_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd079d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d865e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-keras-gpu",
   "language": "python",
   "name": "tensorflow-keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
